{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double DQN - Atari Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117 #CUDA versions of pytorch\n",
    "#!pip install ale_py\n",
    "#!pip install ipywidgets\n",
    "#!pip install --upgrade gym[atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0EEQBsU2gvE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict, deque\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "dueling = True\n",
    "output_rgb = False\n",
    "render_mode = \"rgb_array\"\n",
    "#render_mode = \"human\"\n",
    "\n",
    "env = gym.make('ALE/Breakout-v5', render_mode=render_mode)\n",
    "env.metadata['render_fps'] = 60\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon_decay = 0.0001\n",
    "batch_size = 32\n",
    "lr = 0.0003\n",
    "discount_factor = 0.99\n",
    "update_frequency = 5\n",
    "\n",
    "memory_len = 10000\n",
    "episodes = 100000\n",
    "test_episodes = 100\n",
    "\n",
    "if not os.path.isdir(\"output\"):\n",
    "    os.mkdir(\"output\")\n",
    "\n",
    "if render_mode == \"rgb_array\" and output_rgb is True:\n",
    "    if not os.path.isdir(\"output/images\"):\n",
    "        os.mkdir(\"output/images\")\n",
    "\n",
    "    if not os.path.isdir(\"output/images_test\"):\n",
    "        os.mkdir(\"output/images_test\")\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        if not os.path.isdir(f\"output/images/{episode}\"):\n",
    "            os.mkdir(f\"output/images/{episode}\")\n",
    "\n",
    "    for episode in range(test_episodes):\n",
    "        if not os.path.isdir(f\"output/images_test/{episode}\"):\n",
    "            os.mkdir(f\"output/images_test/{episode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines the neural network structure for Deep Q\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, num_actions, lr):\n",
    "        super(DQNetwork, self).__init__() # nn.Module super class\n",
    "        self.linear_model = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(4, 32, kernel_size=8, stride=4)),\n",
    "            ('relu0', nn.ReLU()),\n",
    "            ('conv1', nn.Conv2d(32, 64, 4, 2)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(64, 64, 3, 1)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('flatten0', nn.Flatten()),\n",
    "            ('dense0', nn.Linear(7*7*64, 512)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('dense1', nn.Linear(512, num_actions))\n",
    "            ]\n",
    "        ))\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    # Performs forward pass in the network\n",
    "    def forwardPass(self, x):\n",
    "        x = self.linear_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNetwork(nn.Module):\n",
    "    def __init__(self, num_actions, lr):\n",
    "        super(DuelingDQNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        std = math.sqrt(2.0 / (4 * 84 * 84))\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=std)\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        std = math.sqrt(2.0 / (32 * 4 * 8 * 8))\n",
    "        nn.init.normal_(self.conv2.weight, mean=0.0, std=std)\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        std = math.sqrt(2.0 / (64 * 32 * 4 * 4))\n",
    "        nn.init.normal_(self.conv3.weight, mean=0.0, std=std)\n",
    "        self.conv3.bias.data.fill_(0.0)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        std = math.sqrt(2.0 / (64 * 64 * 3 * 3))\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=std)\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        self.V = nn.Linear(512, 1)\n",
    "        self.A = nn.Linear(512, num_actions)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    # Performs forward pass in the network\n",
    "    def forwardPass(self, state_size):\n",
    "        x = F.relu(self.conv1(state_size))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1))) # Flatten input\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        Q = V + (A - A.mean(dim=1, keepdim=True))\n",
    "        return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iurxHq-H23g5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initital two DQN networks - one is the policy network and the other is the target network\n",
    "'''\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, epsilon_decay, batch_size, lr, discount_factor, memory_len):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_len)\n",
    "        \n",
    "        self.epsilon = 1.0 #starting epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if dueling is True: # use Double Dueling DQN\n",
    "            self.policy_net = DuelingDQNetwork(self.action_size, self.learning_rate).to(self.device)\n",
    "            self.target_net = DuelingDQNetwork(self.action_size, self.learning_rate).to(self.device)\n",
    "        else: # not dueling, just use double DQN\n",
    "            self.policy_net = DQNetwork(self.action_size, self.learning_rate).to(self.device)\n",
    "            self.target_net = DQNetwork(self.action_size, self.learning_rate).to(self.device)\n",
    "        self.updateTargetPolicy()\n",
    "        \n",
    "    '''\n",
    "    Function definition for choosing an action based on ɛ-greedy policy. \n",
    "        i.e For P(1-ɛ) => argmax(Qt(a))\n",
    "        else for P(ɛ) => random(a)\n",
    "    '''\n",
    "    def getAction(self, current_state_vector):\n",
    "        rand_num = np.random.random()\n",
    "        if rand_num < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            if not torch.is_tensor(current_state_vector):\n",
    "                current_state_vector = torch.from_numpy(current_state_vector).float().to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.forwardPass(current_state_vector).view(-1,)\n",
    "            best_action = torch.argmax(q_values).item()\n",
    "        return best_action\n",
    "    \n",
    "    '''\n",
    "    Update the target network parameters based on the policy network parameters\n",
    "    '''\n",
    "    def updateTargetPolicy(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    '''\n",
    "    Implements epsilon decay\n",
    "    ''' \n",
    "    def updateEpsilon(self):\n",
    "        self.epsilon -= (self.epsilon * self.epsilon_decay)\n",
    "        return self.epsilon\n",
    "    \n",
    "    '''\n",
    "    Apply preprocessing to the captured frame/state\n",
    "    '''\n",
    "    def preprocessState(self, state):\n",
    "        img = state[34:-16, :, :]\n",
    "        resize = T.Compose([\n",
    "                    T.ToPILImage(),\n",
    "                    T.Grayscale(),\n",
    "                    T.Resize((84, 84)),\n",
    "                    T.ToTensor()\n",
    "                ])\n",
    "        img = resize(img)\n",
    "        return img.to(self.device)\n",
    "\n",
    "    '''\n",
    "    Train the neural network\n",
    "    '''\n",
    "    def trainModel(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            current_state = torch.tensor([]).float().to(self.device)\n",
    "            actions = list()\n",
    "            rewards = list()\n",
    "            next_states = torch.tensor([]).float().to(self.device)\n",
    "            dones = list()\n",
    "            \n",
    "            # Append each memory values in the separate lists\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                current_state = torch.cat((current_state, state)).float().to(self.device)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_states = torch.cat((next_states, next_state)).float().to(self.device)\n",
    "                dones.append(done_boolean)\n",
    "            \n",
    "            # Convert each list to torch tensors\n",
    "            actions = torch.from_numpy(np.array(actions)).to(self.device)\n",
    "            rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
    "            dones = torch.from_numpy(np.array(dones)).to(self.device)\n",
    "\n",
    "            # Make a forward pass in the policy network based on the current state\n",
    "            # and choose the q-value of the action\n",
    "            policy_q_values = self.policy_net.forwardPass(current_state).gather(1, actions.view(-1,1).type(torch.int64)).view(-1,)\n",
    "\n",
    "            # Make a forward pass in the target network based on the next state\n",
    "            # and choose the q-value of the action which gives the highest q-value\n",
    "            policy_best_actions = self.policy_net.forwardPass(next_states).argmax(dim=1).view(-1,1)\n",
    "            target_q_values = self.target_net.forwardPass(next_states).gather(1, policy_best_actions).view(-1,)\n",
    "            \n",
    "            # Compute the target\n",
    "            y_target = list()\n",
    "            for index, value in enumerate(target_q_values):\n",
    "                if dones[index]:\n",
    "                    y_target.append(rewards[index])\n",
    "                else:\n",
    "                    y_target.append(rewards[index] + self.discount_factor * value)\n",
    "            \n",
    "            y_target = torch.stack(y_target, dim=0) # Create the list to torch tensor\n",
    "\n",
    "            # Calculate the MSE loss and perform a backward pass in the policy network\n",
    "            criterion = nn.MSELoss()\n",
    "            loss = criterion(y_target, policy_q_values)\n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "    def run(self, env, numEpisodes, isTest):\n",
    "        total_reward_arr = list()\n",
    "        cumulative_reward_arr = list() # Store cumulative reward\n",
    "        frames_arr = list() # Store the num of frames per episode\n",
    "        avg_frames_arr = list() # Store average num of frames every 50 episodes\n",
    "        train_frame = 4\n",
    "\n",
    "        if isTest is True:\n",
    "            self.epsilon = 0  # Agent only chooses greedy actions from the learnt policy\n",
    "\n",
    "        for episode in tqdm(range(numEpisodes)):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            agent_lives = 5\n",
    "            frames = 0\n",
    "            env.reset()\n",
    "\n",
    "            obs, _, _, _, _ = env.step(1) # 1 is FIRE action\n",
    "\n",
    "            current_state = self.preprocessState(obs)\n",
    "            current_state = torch.cat((current_state, current_state, current_state, current_state)).unsqueeze(0) # 1-4-84-84\n",
    "    \n",
    "            while not done:\n",
    "                action = self.getAction(current_state)\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated|truncated\n",
    "\n",
    "                renderOutput = env.render()\n",
    "                if render_mode == \"rgb_array\" and output_rgb is True:\n",
    "                    new_im = Image.fromarray(renderOutput)\n",
    "                    base_dir = \"images_test\" if isTest else \"images\"\n",
    "                    new_im.save(f\"output/{base_dir}/{episode}/frame_{frames+1}.png\")\n",
    "\n",
    "                next_state = self.preprocessState(next_state) # 84-84\n",
    "                next_state_history = torch.cat((current_state.squeeze(0)[1:, :, :], next_state)).unsqueeze(0) # 1-4-84-84\n",
    "\n",
    "                if isTest is False:\n",
    "                    self.memory.append((current_state, action, reward, next_state_history, done)) #Add experience to the experience replay deque. deque removes oldest element when max size is reached.\n",
    "                    if frames % train_frame == 0:\n",
    "                        self.trainModel()\n",
    "\n",
    "                total_reward += reward\n",
    "                frames += 1\n",
    "                        \n",
    "                if agent_lives > info['lives']: # dead\n",
    "                    obs, _, _, _, _ = env.step(1)\n",
    "                    \n",
    "                    obs = self.preprocessState(obs)\n",
    "                    current_state = torch.cat((current_state.squeeze(0)[2:, :, :], next_state, obs)).unsqueeze(0)\n",
    "                    agent_lives = info['lives']\n",
    "                else:\n",
    "                    current_state = next_state_history\n",
    "\n",
    "            frames_arr.append(frames)\n",
    "            # Calculate average frames for every 50 episodes\n",
    "            if (episode+1) % 50 == 0:\n",
    "                avg_frames_arr.append(np.average(frames_arr))\n",
    "                frames_arr.clear()\n",
    "\n",
    "            total_reward_arr.append(total_reward)\n",
    "\n",
    "            if isTest is False:\n",
    "                if episode % update_frequency == 0:\n",
    "                    self.updateTargetPolicy()\n",
    "\n",
    "                ep = self.updateEpsilon()\n",
    "\n",
    "                if np.mean(total_reward_arr[-10:]) > 200:\n",
    "                    break\n",
    "\n",
    "            if len(cumulative_reward_arr) == 0:\n",
    "                cumulative_reward = total_reward\n",
    "            else:\n",
    "                cumulative_reward = cumulative_reward_arr[-1] + total_reward\n",
    "            cumulative_reward_arr.append(cumulative_reward)\n",
    "        \n",
    "        return (total_reward_arr, cumulative_reward_arr, avg_frames_arr) \n",
    "    \n",
    "    '''\n",
    "    Save trained model.\n",
    "    '''\n",
    "    def saveModel(self, filename):\n",
    "        torch.save(self.policy_net.state_dict(), filename)\n",
    "    \n",
    "    '''\n",
    "    Load trained model.\n",
    "    '''\n",
    "    def load_model(self, filename):\n",
    "        self.policy_net.load_state_dict(torch.load(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "cmRnwGENZdoY",
    "outputId": "8f6aeed1-95da-41fc-eea2-93e607e0ee7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(action_size, epsilon_decay, batch_size, lr, discount_factor, memory_len)\n",
    "total_reward_arr, cumulative_reward_arr, avg_frames_arr = agent.run(env, episodes, False)\n",
    "\n",
    "agent.saveModel(\"output/DQN_breakout_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "-jkvfqON3Hsx",
    "outputId": "151a891c-56d9-428d-eae3-5b23897e62e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(total_reward_arr)\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.savefig('output/reward_per_episode.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(cumulative_reward_arr)\n",
    "plt.title('Cumulative Reward Over All Episodes')\n",
    "plt.ylabel('Cumulative Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.savefig('output/cumulative_reward_overall.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(avg_frames_arr)\n",
    "plt.title('Average Frames per 50 Episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Frames')\n",
    "plt.savefig('output/avg_frames_per_50episodes.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.asarray(total_reward_arr).astype(int)\n",
    "c = np.savetxt('output/total_reward_arr.txt', arr, fmt='%i', delimiter='\\n')\n",
    "print(statistics.mean(total_reward_arr))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model weights and test the agent for 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(action_size, epsilon_decay, batch_size, lr, discount_factor, memory_len)\n",
    "agent.load_model(\"output/DQN_breakout_weights.pth\")\n",
    "test_rewards, _, _ = agent.run(env, test_episodes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_rewards)\n",
    "plt.title('Test Total Reward per Episode')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.savefig('output/test_reward_per_episode.png', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Double DQN Breakout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
